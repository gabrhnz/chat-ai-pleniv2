# ğŸ•·ï¸ GuÃ­a de Web Scraping para FAQs

Esta guÃ­a explica cÃ³mo extraer informaciÃ³n automÃ¡ticamente de la web de la UNC y generar FAQs con IA.

---

## ğŸ¯ Â¿QuÃ© hace el sistema?

1. **Scrape** la web oficial de la UNC (https://unc.edu.ve/)
2. **Extrae** contenido relevante (tÃ­tulos, pÃ¡rrafos, listas)
3. **Genera FAQs** automÃ¡ticamente usando GPT-4o-mini
4. **Crea embeddings** para bÃºsqueda semÃ¡ntica
5. **Inserta** las FAQs en la base de datos

---

## ğŸš€ Uso RÃ¡pido

### 1. Scrape la web y genera FAQs

```bash
npm run scrape:web
```

Esto crearÃ¡ un archivo `generated-faqs.json` con las FAQs generadas.

### 2. Revisa las FAQs generadas

Abre `generated-faqs.json` y verifica que las FAQs sean correctas y relevantes.

### 3. Inserta en la base de datos

```bash
npm run insert:scraped
```

---

## âš™ï¸ ConfiguraciÃ³n

### URLs a Scrape

Edita `scripts/scrape-unc-website.js` lÃ­nea ~180:

```javascript
const urls = [
  'https://unc.edu.ve/',
  'https://unc.edu.ve/admisiones',
  'https://unc.edu.ve/carreras',
  'https://unc.edu.ve/becas',
  'https://unc.edu.ve/contacto',
  // Agrega mÃ¡s URLs aquÃ­
];
```

### Personalizar el Prompt

Edita el prompt en `scripts/scrape-unc-website.js` lÃ­nea ~90 para ajustar:
- NÃºmero de FAQs generadas (3-8 por defecto)
- Longitud de respuestas (mÃ¡ximo 20 palabras)
- CategorÃ­as
- Formato de respuestas

---

## ğŸ“Š Ejemplo de Output

```json
{
  "question": "Â¿QuÃ© programas cientÃ­ficos ofrece la UNC?",
  "answer": "Ofrecemos **BiotecnologÃ­a**, **Neurociencia**, y mÃ¡s. ğŸ”¬ Â¿Te interesa alguno en particular?",
  "category": "carreras",
  "keywords": ["programas cientÃ­ficos", "BiotecnologÃ­a"],
  "metadata": {
    "source_url": "https://unc.edu.ve/",
    "source_title": "UNC â€“ Universidad Nacional de las Ciencias...",
    "generated_at": "2025-10-12T23:44:10.667Z"
  }
}
```

---

## ğŸ”§ Troubleshooting

### Error 404 en URLs

Algunas pÃ¡ginas pueden no existir. El scraper las saltarÃ¡ automÃ¡ticamente.

**SoluciÃ³n:** Verifica las URLs correctas en la web de la UNC y actualiza el array de URLs.

### FAQs de baja calidad

Si las FAQs generadas no son buenas:

1. **Mejora el prompt** - SÃ© mÃ¡s especÃ­fico sobre quÃ© tipo de FAQs quieres
2. **Agrega mÃ¡s contexto** - Scrape pÃ¡ginas con mÃ¡s informaciÃ³n
3. **Ajusta la temperatura** - Reduce para respuestas mÃ¡s conservadoras

### Rate Limiting

Si haces muchas requests:

1. El script tiene un delay de 2 segundos entre pÃ¡ginas
2. Puedes aumentarlo en lÃ­nea ~200:
   ```javascript
   await new Promise(resolve => setTimeout(resolve, 5000)); // 5 segundos
   ```

---

## ğŸ¨ PersonalizaciÃ³n Avanzada

### Extraer mÃ¡s tipos de contenido

En `scrapePage()` puedes agregar:

```javascript
// Extraer tablas
$('table').each((i, el) => {
  // ... procesar tablas
});

// Extraer imÃ¡genes con alt text
$('img[alt]').each((i, el) => {
  const alt = $(el).attr('alt');
  // ... procesar imÃ¡genes
});
```

### Filtrar contenido irrelevante

```javascript
// Ignorar secciones especÃ­ficas
$('.ads, .sidebar, .comments').remove();
```

### Generar FAQs en otros idiomas

Modifica el prompt del sistema:

```javascript
content: 'Eres un experto en crear FAQs. Responde en inglÃ©s...'
```

---

## ğŸ“ˆ Mejores PrÃ¡cticas

1. **Revisa siempre** las FAQs generadas antes de insertarlas
2. **Scrape periÃ³dicamente** para mantener la informaciÃ³n actualizada
3. **Archiva los JSONs** generados para referencia futura
4. **Monitorea la calidad** de las respuestas del chatbot

---

## ğŸ”„ AutomatizaciÃ³n

Para scrape automÃ¡tico periÃ³dico, puedes usar:

### Cron Job (Linux/Mac)

```bash
# Scrape cada semana los lunes a las 2am
0 2 * * 1 cd /path/to/project && npm run scrape:web
```

### GitHub Actions

Crea `.github/workflows/scrape.yml`:

```yaml
name: Weekly Scrape
on:
  schedule:
    - cron: '0 2 * * 1'  # Lunes 2am
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: npm install
      - run: npm run scrape:web
      - run: npm run insert:scraped
```

---

## ğŸ’¡ Ideas de ExpansiÃ³n

1. **Scrape PDFs** - Extraer informaciÃ³n de documentos oficiales
2. **Scrape redes sociales** - Instagram, Twitter de la UNC
3. **Scrape noticias** - Mantener FAQs actualizadas con eventos
4. **Multi-idioma** - Generar FAQs en inglÃ©s y espaÃ±ol
5. **ValidaciÃ³n automÃ¡tica** - Verificar que las FAQs no sean duplicadas

---

## ğŸ“š Recursos

- [Cheerio Docs](https://cheerio.js.org/)
- [Axios Docs](https://axios-http.com/)
- [OpenAI API](https://platform.openai.com/docs)

---

## âš ï¸ Consideraciones Legales

- Respeta el `robots.txt` del sitio
- No hagas scraping agresivo (usa delays)
- Verifica los tÃ©rminos de servicio del sitio
- Da crÃ©dito a la fuente original

---

Â¡Happy Scraping! ğŸ•·ï¸âœ¨
